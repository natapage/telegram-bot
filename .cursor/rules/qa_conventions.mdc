---
alwaysApply: true
---

# QA: Правила тестирования

> **TDD**: Test-Driven Development - тесты пишутся ДО реализации

## Принципы

### Основа
- **KISS**: простые, понятные тесты без избыточной сложности
- **DRY**: fixtures для переиспользования, избегать дублирования setup кода
- **One test - one thing**: каждый тест проверяет одну конкретную вещь
- **Test behavior, not implementation**: тестируем ЧТО делает код, не КАК

### Что тестируем
✅ **Критичный функционал**:
- Бизнес-логика (DialogManager: создание истории, обрезка контекста)
- Обработка ошибок (LLMClient: API errors, timeouts)
- Валидация данных (Config: обязательные параметры)
- Публичные методы компонентов

❌ **НЕ тестируем**:
- Boilerplate код (геттеры/сеттеры, простые свойства)
- Приватные методы напрямую (тестируем через публичные)
- Внешние библиотеки (aiogram, openai)
- Очевидные вещи (конструкторы без логики)

### Структура тестов
```
tests/
├── conftest.py          # Общие fixtures (config, managers)
├── unit/                # Unit-тесты компонентов
│   ├── test_config.py
│   ├── test_dialog_manager.py
│   └── test_llm_client.py
└── integration/         # Integration-тесты
    └── test_handler.py
```

## TDD Workflow

### Цикл Red-Green-Refactor
1. **Red**: Написать failing тест для нового функционала
2. **Green**: Написать минимальный код для прохождения теста
3. **Refactor**: Улучшить код, сохраняя прохождение тестов

### Правила
- Новый функционал = сначала тест, потом реализация
- Один цикл = один тест + минимальная реализация
- Не писать реализацию без теста
- Если тест проходит без изменений в коде - тест избыточен

## Naming Convention

### Формат
```python
def test_<метод>_<сценарий>_<результат>():
    """Тест: краткое описание что проверяем"""
```

### Примеры
```python
def test_get_history_creates_new_history_with_system_prompt():
    """Тест: get_history() создаёт новую историю с системным промптом"""

def test_add_message_appends_to_history():
    """Тест: add_message() добавляет сообщение в историю"""

@pytest.mark.asyncio
async def test_generate_response_raises_on_api_error():
    """Тест: generate_response() выбрасывает исключение при ошибке API"""
```

## Fixtures

### Общие fixtures (conftest.py)
```python
@pytest.fixture
def config(monkeypatch):
    """Мокированная конфигурация для тестов"""
    # Setup + return

@pytest.fixture
def dialog_manager(config):
    """DialogManager с тестовой конфигурацией"""
    return DialogManager(config)
```

### Правила
- Переиспользовать fixtures для setup кода
- Не создавать fixtures для одноразовых вещей
- Именовать fixtures понятно (не `dm`, а `dialog_manager`)
- Типизировать возвращаемое значение

## Mocking

### Что мокать
- Внешние API (AsyncOpenAI)
- Переменные окружения (через `monkeypatch`)
- I/O операции (файлы, сеть)

### Что НЕ мокать
- Тестируемый компонент
- Простые зависимости (Config в DialogManager)
- Встроенные типы данных

### Async mocking
```python
from unittest.mock import AsyncMock

@pytest.fixture
def mock_openai():
    mock = AsyncMock()
    mock.chat.completions.create.return_value = MockResponse("AI response")
    return mock
```

## Проверки (Assertions)

### Правила
- Минимум assertions в одном тесте (идеально 1-2)
- Проверять конкретное поведение, не состояние всего объекта
- Использовать точные проверки: `assert x == 5`, не `assert x > 0`

### Примеры
```python
# ✅ Хорошо
assert len(history) == 2
assert history[1]["role"] == "user"

# ❌ Плохо (слишком много проверок)
assert len(history) == 2
assert history[0]["role"] == "system"
assert history[1]["role"] == "user"
assert history[1]["content"] == "Hello"
assert user_id in manager.dialogs
```

## Coverage

### Метрики
- **Unit-тесты**: >= 90% для бизнес-логики (DialogManager, Config)
- **Integration-тесты**: >= 70% для handlers
- **Общий coverage**: >= 80%

### Команды
```bash
make test        # Запуск тестов
make test-cov    # Тесты + coverage report
```

### Приоритеты
1. Критичные компоненты (DialogManager, LLMClient)
2. Обработка ошибок
3. Граничные условия (пустые списки, None, edge cases)
4. Типичные сценарии использования

## Async тесты

### Маркировка
```python
@pytest.mark.asyncio
async def test_async_function():
    result = await some_async_call()
    assert result is not None
```

### Async fixtures
```python
@pytest.fixture
async def async_setup():
    resource = await create_resource()
    yield resource
    await resource.cleanup()
```

## Антипаттерны

❌ **Избегать**:
- Тесты с side effects (изменение глобального состояния)
- Зависимости между тестами (порядок выполнения)
- Тестирование внутренней реализации (private методы)
- Sleep/wait в тестах (использовать mocks)
- Множественные assertions разного поведения
- Тесты "на всякий случай" без понятной цели

## Checklist перед commit

- [ ] Все тесты проходят: `make test`
- [ ] Coverage >= 80%: `make test-cov`
- [ ] Нет warnings от pytest
- [ ] Новый функционал покрыт тестами
- [ ] Тесты независимы и воспроизводимы
- [ ] Имена тестов понятны и описательны

---

**Правило**: Если тест не помогает найти баги или не проверяет критичный функционал - он не нужен!
